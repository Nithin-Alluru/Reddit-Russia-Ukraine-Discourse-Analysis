{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T17:33:22.978792Z",
     "start_time": "2025-05-03T17:33:22.968540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get preprocessing methods from make_network.ipynb to keep consistent.\n",
    "RANDOM_STATE = 5664\n",
    "import os\n",
    "import pandas as pd\n",
    "def load_and_clean_data(filepath):\n",
    "    \"\"\"\n",
    "    Load Reddit data and remove duplicate user-subreddit combinations.\n",
    "    \n",
    "    Args:\n",
    "        filepath (str): Path to the CSV data file\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Cleaned dataframe with unique user-subreddit combinations\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(filepath):\n",
    "            raise FileNotFoundError(f\"Data file not found: {filepath}\")\n",
    "            \n",
    "        # Load the dataset\n",
    "        print(f\"Loading data from {filepath}...\")\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"Original data shape: {df.shape}\")\n",
    "        \n",
    "        # Remove duplicate user-subreddit combinations\n",
    "        df_unique = df.drop_duplicates(subset=['author_name', 'subreddit']).copy()\n",
    "        \n",
    "        # Remove empty texts\n",
    "        df_unique = df_unique.dropna(subset=[\"self_text\", \"post_self_text\", \"post_title\"])\n",
    "        \n",
    "        print(f\"Data shape after cleaning: {df_unique.shape}\")\n",
    "        \n",
    "        return df_unique\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in load_and_clean_data: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "def analyze_post_dates(df):\n",
    "    # Convert post_created_time to datetime\n",
    "    df['post_created_time'] = pd.to_datetime(df['post_created_time'])\n",
    "    \n",
    "    # Get the earliest and latest dates\n",
    "    min_date = df['post_created_time'].min()\n",
    "    max_date = df['post_created_time'].max()\n",
    "    \n",
    "    return min_date, max_date\n",
    "\n",
    "def filter_by_date(df, start_date=None, end_date=None):\n",
    "    \"\"\"\n",
    "    Filter dataframe to include only posts within a specific date range.\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame with 'post_created_time' column\n",
    "        start_date (str, datetime, optional): Keep posts on or after this date\n",
    "        end_date (str, datetime, optional): Keep posts on or before this date\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Filtered dataframe\n",
    "    \"\"\"\n",
    "    # Make sure post_created_time is datetime\n",
    "    if not pd.api.types.is_datetime64_dtype(df['post_created_time']):\n",
    "        df['post_created_time'] = pd.to_datetime(df['post_created_time'])\n",
    "    \n",
    "    original_count = len(df)\n",
    "    \n",
    "    # Apply date filters\n",
    "    if start_date is not None:\n",
    "        if isinstance(start_date, str):\n",
    "            start_date = pd.to_datetime(start_date)\n",
    "        df = df[df['post_created_time'] >= start_date]\n",
    "    \n",
    "    if end_date is not None:\n",
    "        if isinstance(end_date, str):\n",
    "            end_date = pd.to_datetime(end_date)\n",
    "        df = df[df['post_created_time'] <= end_date]\n",
    "    \n",
    "    # Report on filtering\n",
    "    print(f\"Date filtering:\")\n",
    "    if start_date is not None:\n",
    "        print(f\"  Start date: {start_date}\")\n",
    "    if end_date is not None:\n",
    "        print(f\"  End date: {end_date}\")\n",
    "    print(f\"  Original records: {original_count}\")\n",
    "    print(f\"  Filtered records: {len(df)} ({len(df)/original_count*100:.1f}% retained)\")\n",
    "    \n",
    "    return df"
   ],
   "id": "189120c123e833e4",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load data",
   "id": "16770055ed9fabad"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-03T17:34:42.078391Z",
     "start_time": "2025-05-03T17:33:22.989765Z"
    }
   },
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "df_clean = load_and_clean_data(\"reddit_opinion_ru_ua.csv\")\n",
    "min_date, max_date = analyze_post_dates(df_clean)\n",
    "cutoff_date = max_date - timedelta(days=10)\n",
    "\n",
    "df_recent = filter_by_date(df_clean, start_date=cutoff_date)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from reddit_opinion_ru_ua.csv...\n",
      "Original data shape: (5168018, 24)\n",
      "Data shape after cleaning: (118477, 24)\n",
      "Date filtering:\n",
      "  Start date: 2025-04-19 10:05:00\n",
      "  Original records: 118477\n",
      "  Filtered records: 7297 (6.2% retained)\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T17:34:42.218888Z",
     "start_time": "2025-05-03T17:34:42.197037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "todrop = [\"comment_id\", \"created_time\",\"post_id\",\"user_is_verified\",\"user_account_created_time\", \"user_awardee_karma\", \"user_awarder_karma\", \"user_comment_karma\", \"user_link_karma\", \"post_created_time\"]\n",
    "df_recent.drop(todrop, axis=1)"
   ],
   "id": "80ce2289a8e5840c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       score                                          self_text  \\\n",
       "0          1  I'd have to agree that it's hard to shop for m...   \n",
       "9          1  It looks like a complicated procedure. In a te...   \n",
       "11         1  It is atmospheric phenomena, we're due another...   \n",
       "43         1  Nope.  Plasma penetration.  Viewable on multip...   \n",
       "49         1  It's not the simple men, but the women who don...   \n",
       "...      ...                                                ...   \n",
       "98590      2  J'ai une offre de thése en IA mais je ne suis ...   \n",
       "98633     20                                       Bots are mad   \n",
       "98643     78  Trump loves to take credit for operation warp ...   \n",
       "98674    -10                    Why does Trump love him much?!!   \n",
       "98764      7  He's not completely protected. The individual ...   \n",
       "\n",
       "                   subreddit           author_name  controversiality  ups  \\\n",
       "0                AskARussian              rsaachit                 0    1   \n",
       "9      UkraineWarVideoReport    Spare-Sandwich8848                 0    1   \n",
       "11                conspiracy     South-Rabbit-4064                 0    1   \n",
       "43                conspiracy             fr33lancr                 0    1   \n",
       "49               AskARussian         Basic_Ad_2235                 0    1   \n",
       "...                      ...                   ...               ...  ...   \n",
       "98590                 france    No-Psychology-7771                 0    2   \n",
       "98633             conspiracy      mykidsnever_call                 0   20   \n",
       "98643             conspiracy          moanysopran0                 0   78   \n",
       "98674             conspiracy  General-Priority-479                 0  -10   \n",
       "98764             conspiracy             cjweisman                 1    7   \n",
       "\n",
       "       downs  user_total_karma  post_score  \\\n",
       "0          0            2272.0           8   \n",
       "9          0             740.0         155   \n",
       "11         0           92672.0         214   \n",
       "43         0           15888.0         214   \n",
       "49         0           10827.0           8   \n",
       "...      ...               ...         ...   \n",
       "98590      0             100.0           0   \n",
       "98633      0           11222.0         341   \n",
       "98643      0           17167.0         341   \n",
       "98674      0           14141.0         341   \n",
       "98764      0          160926.0         341   \n",
       "\n",
       "                                          post_self_text  \\\n",
       "0      hello!! I’m currently trying to come up with g...   \n",
       "9      Video was edited by the source.\\n\\nThis video ...   \n",
       "11     The massive blackout that hit Spain, Portugal,...   \n",
       "43     The massive blackout that hit Spain, Portugal,...   \n",
       "49     hello!! I’m currently trying to come up with g...   \n",
       "...                                                  ...   \n",
       "98590  Bonjour à tous,\\n\\nJe suis récemment diplômé e...   \n",
       "98633  [**Guardian**](https://archive.is/2tA6J) — The...   \n",
       "98643  [**Guardian**](https://archive.is/2tA6J) — The...   \n",
       "98674  [**Guardian**](https://archive.is/2tA6J) — The...   \n",
       "98764  [**Guardian**](https://archive.is/2tA6J) — The...   \n",
       "\n",
       "                                              post_title  post_upvote_ratio  \\\n",
       "0               Gifts for Russian man - from an American               0.78   \n",
       "9      NSFW/NSFL: 2 Russian soldiers shot themselves....               0.97   \n",
       "11     Spain and Portugal Blackout due to 'Atmospheri...               0.82   \n",
       "43     Spain and Portugal Blackout due to 'Atmospheri...               0.82   \n",
       "49              Gifts for Russian man - from an American               0.78   \n",
       "...                                                  ...                ...   \n",
       "98590  Quel domaine IT choisir en tant que jeune dipl...               0.20   \n",
       "98633  Trump pretty much accused Fauci of crimes agai...               0.72   \n",
       "98643  Trump pretty much accused Fauci of crimes agai...               0.72   \n",
       "98674  Trump pretty much accused Fauci of crimes agai...               0.72   \n",
       "98764  Trump pretty much accused Fauci of crimes agai...               0.72   \n",
       "\n",
       "       post_thumbs_ups  post_total_awards_received  \n",
       "0                    8                           0  \n",
       "9                  155                           0  \n",
       "11                 214                           0  \n",
       "43                 214                           0  \n",
       "49                   8                           0  \n",
       "...                ...                         ...  \n",
       "98590                0                           0  \n",
       "98633              341                           0  \n",
       "98643              341                           0  \n",
       "98674              341                           0  \n",
       "98764              341                           0  \n",
       "\n",
       "[7297 rows x 14 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>self_text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author_name</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>user_total_karma</th>\n",
       "      <th>post_score</th>\n",
       "      <th>post_self_text</th>\n",
       "      <th>post_title</th>\n",
       "      <th>post_upvote_ratio</th>\n",
       "      <th>post_thumbs_ups</th>\n",
       "      <th>post_total_awards_received</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I'd have to agree that it's hard to shop for m...</td>\n",
       "      <td>AskARussian</td>\n",
       "      <td>rsaachit</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2272.0</td>\n",
       "      <td>8</td>\n",
       "      <td>hello!! I’m currently trying to come up with g...</td>\n",
       "      <td>Gifts for Russian man - from an American</td>\n",
       "      <td>0.78</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>It looks like a complicated procedure. In a te...</td>\n",
       "      <td>UkraineWarVideoReport</td>\n",
       "      <td>Spare-Sandwich8848</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>740.0</td>\n",
       "      <td>155</td>\n",
       "      <td>Video was edited by the source.\\n\\nThis video ...</td>\n",
       "      <td>NSFW/NSFL: 2 Russian soldiers shot themselves....</td>\n",
       "      <td>0.97</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>It is atmospheric phenomena, we're due another...</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>South-Rabbit-4064</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>92672.0</td>\n",
       "      <td>214</td>\n",
       "      <td>The massive blackout that hit Spain, Portugal,...</td>\n",
       "      <td>Spain and Portugal Blackout due to 'Atmospheri...</td>\n",
       "      <td>0.82</td>\n",
       "      <td>214</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>Nope.  Plasma penetration.  Viewable on multip...</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>fr33lancr</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15888.0</td>\n",
       "      <td>214</td>\n",
       "      <td>The massive blackout that hit Spain, Portugal,...</td>\n",
       "      <td>Spain and Portugal Blackout due to 'Atmospheri...</td>\n",
       "      <td>0.82</td>\n",
       "      <td>214</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>It's not the simple men, but the women who don...</td>\n",
       "      <td>AskARussian</td>\n",
       "      <td>Basic_Ad_2235</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10827.0</td>\n",
       "      <td>8</td>\n",
       "      <td>hello!! I’m currently trying to come up with g...</td>\n",
       "      <td>Gifts for Russian man - from an American</td>\n",
       "      <td>0.78</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98590</th>\n",
       "      <td>2</td>\n",
       "      <td>J'ai une offre de thése en IA mais je ne suis ...</td>\n",
       "      <td>france</td>\n",
       "      <td>No-Psychology-7771</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Bonjour à tous,\\n\\nJe suis récemment diplômé e...</td>\n",
       "      <td>Quel domaine IT choisir en tant que jeune dipl...</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98633</th>\n",
       "      <td>20</td>\n",
       "      <td>Bots are mad</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>mykidsnever_call</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>11222.0</td>\n",
       "      <td>341</td>\n",
       "      <td>[**Guardian**](https://archive.is/2tA6J) — The...</td>\n",
       "      <td>Trump pretty much accused Fauci of crimes agai...</td>\n",
       "      <td>0.72</td>\n",
       "      <td>341</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98643</th>\n",
       "      <td>78</td>\n",
       "      <td>Trump loves to take credit for operation warp ...</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>moanysopran0</td>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>17167.0</td>\n",
       "      <td>341</td>\n",
       "      <td>[**Guardian**](https://archive.is/2tA6J) — The...</td>\n",
       "      <td>Trump pretty much accused Fauci of crimes agai...</td>\n",
       "      <td>0.72</td>\n",
       "      <td>341</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98674</th>\n",
       "      <td>-10</td>\n",
       "      <td>Why does Trump love him much?!!</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>General-Priority-479</td>\n",
       "      <td>0</td>\n",
       "      <td>-10</td>\n",
       "      <td>0</td>\n",
       "      <td>14141.0</td>\n",
       "      <td>341</td>\n",
       "      <td>[**Guardian**](https://archive.is/2tA6J) — The...</td>\n",
       "      <td>Trump pretty much accused Fauci of crimes agai...</td>\n",
       "      <td>0.72</td>\n",
       "      <td>341</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98764</th>\n",
       "      <td>7</td>\n",
       "      <td>He's not completely protected. The individual ...</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>cjweisman</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>160926.0</td>\n",
       "      <td>341</td>\n",
       "      <td>[**Guardian**](https://archive.is/2tA6J) — The...</td>\n",
       "      <td>Trump pretty much accused Fauci of crimes agai...</td>\n",
       "      <td>0.72</td>\n",
       "      <td>341</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7297 rows × 14 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T17:34:42.387801Z",
     "start_time": "2025-05-03T17:34:42.374526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get basic information\n",
    "print(\"Number of posts:\", len(df_recent))\n",
    "print(\"All subreddits:\")\n",
    "pd.DataFrame(df_recent.subreddit.explode().unique())"
   ],
   "id": "42ce1e145d402330",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts: 7297\n",
      "All subreddits:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                        0\n",
       "0             AskARussian\n",
       "1   UkraineWarVideoReport\n",
       "2              conspiracy\n",
       "3    RussiaUkraineWar2022\n",
       "4           CombatFootage\n",
       "5     UkraineRussiaReport\n",
       "6         FreedomofRussia\n",
       "7              EndlessWar\n",
       "8                 ukraine\n",
       "9         UkraineConflict\n",
       "10               Military\n",
       "11           MilitaryPorn\n",
       "12     NonCredibleDefense\n",
       "13             neoliberal\n",
       "14              worldnews\n",
       "15  UkraineInvasionVideos\n",
       "16         LoveForUkraine\n",
       "17   volunteersForUkraine\n",
       "18           Conservative\n",
       "19            geopolitics\n",
       "20      UkraineWarReports\n",
       "21                 france\n",
       "22           WayOfTheBern\n",
       "23         UkraineWarRoom\n",
       "24     ANormalDayInRussia\n",
       "25                ukraina"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AskARussian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UkraineWarVideoReport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>conspiracy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RussiaUkraineWar2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CombatFootage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UkraineRussiaReport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FreedomofRussia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>EndlessWar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ukraine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>UkraineConflict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Military</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MilitaryPorn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NonCredibleDefense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>neoliberal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>UkraineInvasionVideos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LoveForUkraine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>volunteersForUkraine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Conservative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>geopolitics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>UkraineWarReports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>france</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>WayOfTheBern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>UkraineWarRoom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ANormalDayInRussia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ukraina</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Set up for topic modeling and sentiment analysis for comments, posts, and post titles",
   "id": "a3abd9af5e25d4ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T17:34:42.510594Z",
     "start_time": "2025-05-03T17:34:42.499804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download(\"punkt_tab\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "def preprocess_one_doc(text: str, lower: bool, punct: bool, stop: bool, stem: bool):\n",
    "    puncts = set(string.punctuation)\n",
    "    puncts.add(\"...\") # punct does not contain ellipses\n",
    "    puncts.add(\"…\")\n",
    "    puncts.add(\"===\")\n",
    "    puncts.add(\"—\")\n",
    "    puncts.add(\"–\")\n",
    "    puncts.add(\"“\")\n",
    "    puncts.add(\"”\")\n",
    "    puncts.add(\"’\")\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    # stops.add(\"\")\n",
    "    \n",
    "    \n",
    "    # Lowercase the words depending on sentiment or topic modeling\n",
    "    if lower:\n",
    "        step0 = text.lower()\n",
    "    else:\n",
    "        step0 = text\n",
    "    step1 = word_tokenize(step0)\n",
    "    \n",
    "    \n",
    "    if punct:\n",
    "        step2 = \"\".join([ch for ch in \" \".join(step1) if ch not in puncts]).split()\n",
    "    else:\n",
    "        step2 = step1\n",
    "        \n",
    "        \n",
    "    \n",
    "    if stop:\n",
    "        # Remove stopwords\n",
    "        step3 = [token for token in step2\n",
    "                    if token not in stops # drop stopwords\n",
    "                    # and len(token) > 2 # drop words of insignificant length\n",
    "                    and (not token.startswith(\"http\"))] # drop links\n",
    "    else:\n",
    "        step3 = step2\n",
    "        \n",
    "    if stem:\n",
    "        stemmer = PorterStemmer()\n",
    "        step4 = [stemmer.stem(i) for i in step3]\n",
    "    else:\n",
    "        step4 = step3\n",
    "        \n",
    "    return step4\n",
    "\n",
    "from gensim import corpora, models\n",
    "\n",
    "def make_dictionary(alltexts):\n",
    "    d = corpora.Dictionary(alltexts)\n",
    "    d.filter_extremes(no_below=5, no_above=0.3) # Keep words that are in more than 5 documents, but not in more than a third of all documents\n",
    "    d.compactify()\n",
    "    return d\n",
    "\n",
    "#TODO: Filter out non-english text\n",
    "#TODO: Dropna here instead of at cleanup to preserve data?\n",
    "def make_all_components(df, text_column_name):\n",
    "    # Create with standard preprocessing\n",
    "    preprocessed = df[text_column_name].apply(lambda x: preprocess_one_doc(x, lower=True, stop=True, punct=True, stem=True)) # Preprocess all docs\n",
    "    dictionary = make_dictionary(preprocessed.tolist()) # Use list of lists of strings\n",
    "    corpus = [dictionary.doc2bow(text) for text in preprocessed.tolist()] # bag of words representation of documents\n",
    "    return preprocessed, dictionary, corpus"
   ],
   "id": "e3ce9e99aa141aca",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T17:35:22.047960Z",
     "start_time": "2025-05-03T17:34:42.594550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create with standard preprocessing\n",
    "preprocessed_comments, dictionary_comments, corpus_comments = make_all_components(df_recent, \"self_text\")\n",
    "preprocessed_post_content, dictionary_post_content, corpus_post_content = make_all_components(df_recent, \"post_self_text\")\n",
    "preprocessed_title, dictionary_title, corpus_title = make_all_components(df_recent, \"post_title\")"
   ],
   "id": "c55303aacf345955",
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T17:35:22.068333Z",
     "start_time": "2025-05-03T17:35:22.060433Z"
    }
   },
   "cell_type": "code",
   "source": "preprocessed_comments",
   "id": "e8147f7807cbddb5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [agre, hard, shop, men, unless, nich, hobbi, l...\n",
       "9        [look, like, complic, procedur, technic, sens,...\n",
       "11       [atmospher, phenomena, due, anoth, carrington,...\n",
       "43       [nope, plasma, penetr, viewabl, multipl, sourc...\n",
       "49                         [simpl, men, women, nt, imagin]\n",
       "                               ...                        \n",
       "98590    [jai, une, offr, de, thése, en, ia, mai, je, n...\n",
       "98633                                           [bot, mad]\n",
       "98643    [trump, love, take, credit, oper, warp, speed,...\n",
       "98674                                  [trump, love, much]\n",
       "98764              [complet, protect, individu, state, go]\n",
       "Name: self_text, Length: 7297, dtype: object"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluate to find best number of topics",
   "id": "fcf9da0fb2263625"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T17:35:22.222307Z",
     "start_time": "2025-05-03T17:35:22.214296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Taken from https://medium.com/analytics-vidhya/topic-modeling-using-gensim-lda-in-python-48eaa2344920\n",
    "\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def eval_model(model, text, dic, corp):\n",
    "    # Compute Perplexity\n",
    "    perp = model.log_perplexity(corp)\n",
    "    # a measure of how good the model is. lower the better.\n",
    "    \n",
    "    # Compute Coherence Score\n",
    "    coherence_model_lda = CoherenceModel(model=model, texts=text, dictionary=dic, coherence='c_v')\n",
    "    coherence = coherence_model_lda.get_coherence()\n",
    "    \n",
    "    return perp, coherence\n",
    "\n",
    "def plot_evals(perps, coherences, ks):\n",
    "    \n",
    "    fig = plt.figure(\"Perplexity and Coherence Analysis\", figsize=(8, 8))\n",
    "    axgrid = fig.add_gridspec(1, 2)\n",
    "    \n",
    "    ax1 = fig.add_subplot(axgrid[0, 0])\n",
    "    ax1.plot(ks, perps)\n",
    "    ax1.set_title(\"Number of topics vs Perplexity Score\")\n",
    "    ax1.set_xlabel(\"Number of Topics\")\n",
    "    ax1.set_ylabel(\"Perplexity Score\")\n",
    "    ax1.grid()\n",
    "    \n",
    "    ax2 = fig.add_subplot(axgrid[0, 1])\n",
    "    ax2.plot(ks, coherences)\n",
    "    ax2.set_title(\"Number of topics vs Coherence Score\")\n",
    "    ax2.set_xlabel(\"Number of Topics\")\n",
    "    ax2.set_ylabel(\"Coherence Score\")\n",
    "    ax2.grid()\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def full_model_test_loop(text, corpus, dictionary):\n",
    "    ks = [1,2,3,4,5,6,7,8,9,10, 20, 30]\n",
    "    perps = []\n",
    "    coherences = []\n",
    "    for k in ks:\n",
    "        ldamodel = models.ldamodel.LdaModel(corpus, num_topics=k, id2word=dictionary, passes=20, random_state=RANDOM_STATE)\n",
    "        scores = eval_model(ldamodel, text.tolist(), dictionary, corpus)\n",
    "        perps.append(scores[0])\n",
    "        coherences.append(scores[1])\n",
    "    plot_evals(perps, coherences, ks)"
   ],
   "id": "ffc8ece3411fbfe4",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T17:35:22.325925Z",
     "start_time": "2025-05-03T17:35:22.321550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print(\"Comments eval:\")\n",
    "# full_model_test_loop(preprocessed_comments, corpus_comments, dictionary_comments)\n",
    "# print(\"Post Content eval:\")\n",
    "# full_model_test_loop(preprocessed_post_content, corpus_post_content, dictionary_post_content)\n",
    "# print(\"Post Title eval:\")\n",
    "# full_model_test_loop(preprocessed_title , corpus_title, dictionary_title)\n",
    "\n",
    "# 22 minutes to run all"
   ],
   "id": "a346c5d2f3e50ae0",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Topic Modeling",
   "id": "d7495206ee602d2c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T17:37:27.252631Z",
     "start_time": "2025-05-03T17:35:22.370808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ldamodel_comments = models.ldamodel.LdaModel(corpus_comments, num_topics=10, id2word=dictionary_comments, passes=20, random_state=RANDOM_STATE)\n",
    "ldamodel_post_content = models.ldamodel.LdaModel(corpus_post_content, num_topics=5, id2word=dictionary_post_content, passes=20, random_state=RANDOM_STATE)\n",
    "ldamodel_title = models.ldamodel.LdaModel(corpus_title, num_topics=20, id2word=dictionary_title, passes=20, random_state=RANDOM_STATE)\n",
    "\n",
    "# TODO: call eval_model to get the perplexity and coherence of the top k model"
   ],
   "id": "6dec4df67d883cf8",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T17:37:27.383399Z",
     "start_time": "2025-05-03T17:37:27.376417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Comment topics:\")\n",
    "ldamodel_comments.show_topics()"
   ],
   "id": "f0e5cfaa84ba6d46",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment topics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.044*\"russia\" + 0.028*\"ukrain\" + 0.021*\"war\" + 0.020*\"us\" + 0.019*\"would\" + 0.014*\"countri\" + 0.012*\"nt\" + 0.011*\"want\" + 0.011*\"russian\" + 0.009*\"trump\"'),\n",
       " (1,\n",
       "  '0.022*\"like\" + 0.022*\"fuck\" + 0.015*\"love\" + 0.014*\"peopl\" + 0.011*\"trump\" + 0.009*\"differ\" + 0.009*\"said\" + 0.009*\"thing\" + 0.009*\"want\" + 0.008*\"see\"'),\n",
       " (2,\n",
       "  '0.015*\"like\" + 0.015*\"peopl\" + 0.013*\"russian\" + 0.012*\"nt\" + 0.012*\"go\" + 0.012*\"also\" + 0.010*\"good\" + 0.009*\"moscow\" + 0.008*\"ask\" + 0.007*\"see\"'),\n",
       " (3,\n",
       "  '0.039*\"и\" + 0.038*\"в\" + 0.034*\"news\" + 0.032*\"не\" + 0.019*\"что\" + 0.018*\"это\" + 0.018*\"на\" + 0.017*\"fox\" + 0.017*\"na\" + 0.016*\"ai\"'),\n",
       " (4,\n",
       "  '0.063*\"nt\" + 0.018*\"peopl\" + 0.017*\"think\" + 0.014*\"get\" + 0.014*\"say\" + 0.013*\"ca\" + 0.011*\"even\" + 0.010*\"govern\" + 0.010*\"would\" + 0.010*\"work\"'),\n",
       " (5,\n",
       "  '0.015*\"drone\" + 0.011*\"would\" + 0.010*\"ukrainian\" + 0.010*\"happen\" + 0.010*\"militari\" + 0.008*\"ukrain\" + 0.008*\"never\" + 0.007*\"way\" + 0.007*\"time\" + 0.006*\"year\"'),\n",
       " (6,\n",
       "  '0.018*\"thank\" + 0.016*\"shit\" + 0.013*\"year\" + 0.012*\"time\" + 0.011*\"3\" + 0.011*\"make\" + 0.010*\"account\" + 0.009*\"trump\" + 0.008*\"right\" + 0.008*\"month\"'),\n",
       " (7,\n",
       "  '0.017*\"comment\" + 0.014*\"read\" + 0.013*\"nice\" + 0.013*\"de\" + 0.012*\"post\" + 0.010*\"get\" + 0.010*\"need\" + 0.009*\"le\" + 0.008*\"fire\" + 0.007*\"hous\"'),\n",
       " (8,\n",
       "  '0.052*\"russian\" + 0.044*\"like\" + 0.016*\"name\" + 0.015*\"peopl\" + 0.011*\"look\" + 0.011*\"also\" + 0.011*\"one\" + 0.010*\"know\" + 0.009*\"russia\" + 0.009*\"live\"'),\n",
       " (9,\n",
       "  '0.016*\"one\" + 0.015*\"amp\" + 0.010*\"russian\" + 0.010*\"war\" + 0.009*\"putin\" + 0.009*\"ukrain\" + 0.009*\"go\" + 0.009*\"day\" + 0.009*\"use\" + 0.008*\"get\"')]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T17:37:27.561363Z",
     "start_time": "2025-05-03T17:37:27.553385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Post content topics:\")\n",
    "ldamodel_post_content.show_topics()"
   ],
   "id": "f8308c939e5c1dda",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post content topics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.019*\"banksi\" + 0.015*\"nt\" + 0.012*\"artist\" + 0.009*\"rob\" + 0.007*\"oper\" + 0.007*\"work\" + 0.007*\"never\" + 0.007*\"one\" + 0.006*\"would\" + 0.006*\"art\"'),\n",
       " (1,\n",
       "  '0.014*\"like\" + 0.013*\"would\" + 0.012*\"russia\" + 0.010*\"nt\" + 0.009*\"think\" + 0.007*\"peopl\" + 0.007*\"lawn\" + 0.006*\"one\" + 0.006*\"someth\" + 0.006*\"know\"'),\n",
       " (2,\n",
       "  '0.009*\"russia\" + 0.008*\"would\" + 0.007*\"ukrainian\" + 0.007*\"amp\" + 0.006*\"one\" + 0.006*\"ukrain\" + 0.006*\"like\" + 0.005*\"year\" + 0.005*\"want\" + 0.005*\"time\"'),\n",
       " (3,\n",
       "  '0.035*\"de\" + 0.023*\"la\" + 0.020*\"et\" + 0.019*\"le\" + 0.018*\"soldier\" + 0.014*\"un\" + 0.013*\"l\" + 0.010*\"colombian\" + 0.008*\"roman\" + 0.008*\"en\"'),\n",
       " (4,\n",
       "  '0.020*\"ukrain\" + 0.017*\"war\" + 0.017*\"trump\" + 0.015*\"us\" + 0.010*\"video\" + 0.009*\"inform\" + 0.008*\"sourc\" + 0.008*\"russia\" + 0.007*\"presid\" + 0.007*\"militari\"')]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T17:37:27.670512Z",
     "start_time": "2025-05-03T17:37:27.662534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Title topics:\")\n",
    "ldamodel_title.show_topics()"
   ],
   "id": "f99352b2466c1852",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title topics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(18,\n",
       "  '0.095*\"ukrain\" + 0.081*\"russia\" + 0.053*\"fight\" + 0.045*\"war\" + 0.043*\"trump\" + 0.041*\"forc\" + 0.036*\"fighter\" + 0.036*\"zelenskyy\" + 0.032*\"europ\" + 0.024*\"jet\"'),\n",
       " (10,\n",
       "  '0.140*\"think\" + 0.112*\"russia\" + 0.093*\"north\" + 0.083*\"war\" + 0.071*\"korean\" + 0.037*\"kursk\" + 0.027*\"ru\" + 0.027*\"pov\" + 0.024*\"warfar\" + 0.020*\"fought\"'),\n",
       " (6,\n",
       "  '0.058*\"x\" + 0.057*\"reach\" + 0.055*\"still\" + 0.044*\"zelenski\" + 0.043*\"routin\" + 0.039*\"amaz\" + 0.036*\"1280\" + 0.029*\"deploy\" + 0.025*\"soldier\" + 0.024*\"probabl\"'),\n",
       " (5,\n",
       "  '0.146*\"trump\" + 0.115*\"ukrain\" + 0.082*\"news\" + 0.064*\"war\" + 0.061*\"accus\" + 0.056*\"end\" + 0.051*\"us\" + 0.046*\"peopl\" + 0.041*\"russia\" + 0.034*\"back\"'),\n",
       " (3,\n",
       "  '0.064*\"moscow\" + 0.049*\"ukrainian\" + 0.048*\"region\" + 0.034*\"troop\" + 0.030*\"pov\" + 0.026*\"forc\" + 0.022*\"offici\" + 0.022*\"militari\" + 0.021*\"accord\" + 0.021*\"russia\"'),\n",
       " (12,\n",
       "  '0.076*\"say\" + 0.066*\"ukrain\" + 0.062*\"crimea\" + 0.038*\"new\" + 0.035*\"struck\" + 0.032*\"occupi\" + 0.029*\"wound\" + 0.028*\"univers\" + 0.026*\"armor\" + 0.024*\"retak\"'),\n",
       " (0,\n",
       "  '0.084*\"sub\" + 0.069*\"use\" + 0.058*\"name\" + 0.050*\"come\" + 0.044*\"ahead\" + 0.043*\"toward\" + 0.040*\"ukrain\" + 0.035*\"resid\" + 0.034*\"pelosi\" + 0.033*\"zelenski\"'),\n",
       " (1,\n",
       "  '0.074*\"part\" + 0.051*\"ukrainian\" + 0.047*\"drone\" + 0.042*\"time\" + 0.039*\"fpv\" + 0.035*\"2\" + 0.034*\"mig29\" + 0.032*\"american\" + 0.031*\"carri\" + 0.030*\"april\"'),\n",
       " (14,\n",
       "  '0.067*\"pope\" + 0.059*\"zelenski\" + 0.058*\"funer\" + 0.056*\"seat\" + 0.044*\"took\" + 0.044*\"front\" + 0.044*\"america\" + 0.039*\"get\" + 0.034*\"great\" + 0.033*\"grass\"'),\n",
       " (16,\n",
       "  '0.134*\"putin\" + 0.098*\"would\" + 0.089*\"ukrain\" + 0.051*\"us\" + 0.050*\"presid\" + 0.046*\"life\" + 0.043*\"russia\" + 0.038*\"ua\" + 0.038*\"report\" + 0.037*\"pov\"')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sentiment Analysis",
   "id": "b38d76ec33aa551a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T18:41:04.465614Z",
     "start_time": "2025-05-03T18:41:04.456003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "av = 0\n",
    "for text in df_recent[\"self_text\"]:\n",
    "    av+=len(text)\n",
    "av/len(df_recent[\"self_text\"])"
   ],
   "id": "280fc31b754dc170",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164.58681649993147"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 95
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "#TODO adapt to generic scores, include average pos, neg, neu scores, plot histogram of scores, plot distribution of compound scores",
   "id": "2adad60b440bfdb7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def find_compound_sentiment(document):\n",
    "    sentences = sent_tokenize(document)\n",
    "    s = 0\n",
    "    for sentence in sentences:\n",
    "        sentiment = sid.polarity_scores(sentence)[\"compound\"]\n",
    "        s += sentiment\n",
    "    return s/len(sentences)\n",
    "    \n",
    "def find_all_topic_sentiments(corp, raws, model, refs):\n",
    "    dominant_topics = []\n",
    "    document_scores = []\n",
    "    corp_plus_raws = zip(corp, raws)\n",
    "    for bag, raw in corp_plus_raws:\n",
    "        topics = model.get_document_topics(bag)\n",
    "        dominant_topic = sorted(topics, key=lambda x: -x[1])[0][0]\n",
    "        dominant_topics.append(dominant_topic)\n",
    "        document_scores.append(find_compound_sentiment(raw))\n",
    "        \n",
    "    topic_sentiments = {}\n",
    "    topic_refs = {}\n",
    "    document_topic_score = list(zip(dominant_topics, document_scores, refs))\n",
    "    for i in range(k):\n",
    "        sentiments_in_topic = [sentiment for topic, sentiment, _ in document_topic_score if topic == i]\n",
    "        topic_sentiments[i] = sum(sentiments_in_topic)/len(sentiments_in_topic)\n",
    "        refs_in_topic = [refs for topic, _, refs in document_topic_score if topic == i]\n",
    "        topic_refs[i] = sum(refs_in_topic)/len(refs_in_topic)\n",
    "        \n",
    "    return [topic_sentiments, document_scores, topic_refs]"
   ],
   "id": "f69da25e1750478b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "topic_scores, dataset_scores, topic_nrefs = find_all_topic_sentiments(corpus, dataset[\"content\"], ldamodel, dataset[\"nrefs\"])",
   "id": "332401e1cfd1b731"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
