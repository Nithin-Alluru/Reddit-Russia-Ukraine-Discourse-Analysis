{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Network Analysis\n",
    "\n",
    "This notebook processes a Reddit dataset to create a network where:\n",
    "- Nodes are Reddit users\n",
    "- Edges connect users who have posted to the same subreddit\n",
    "\n",
    "The dataset used is \"reddit_opinion_ru_ua.csv\" from Kaggle (downloaded locally).\n",
    "https://www.kaggle.com/datasets/asaniczka/public-opinion-russia-ukraine-war-updated-daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from reddit_opinion_ru_ua.csv...\n",
      "Original data shape: (4928897, 24)\n",
      "Data shape after cleaning: (736526, 24)\n"
     ]
    }
   ],
   "source": [
    "def load_and_clean_data(filepath):\n",
    "    \"\"\"\n",
    "    Load Reddit data and remove duplicate user-subreddit combinations.\n",
    "    \n",
    "    Args:\n",
    "        filepath (str): Path to the CSV data file\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Cleaned dataframe with unique user-subreddit combinations\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(filepath):\n",
    "            raise FileNotFoundError(f\"Data file not found: {filepath}\")\n",
    "            \n",
    "        # Load the dataset\n",
    "        print(f\"Loading data from {filepath}...\")\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"Original data shape: {df.shape}\")\n",
    "        \n",
    "        # Remove duplicate user-subreddit combinations\n",
    "        df_unique = df.drop_duplicates(subset=['author_name', 'subreddit'])\n",
    "        print(f\"Data shape after cleaning: {df_unique.shape}\")\n",
    "        \n",
    "        return df_unique\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in load_and_clean_data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Load and clean the dataset\n",
    "df_clean = load_and_clean_data(\"reddit_opinion_ru_ua.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Users by Subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 5.0% of users from each subreddit\n",
      "Example of sampled subreddits:\n",
      "  ANormalDayInRussia: 506 users\n",
      "  ArtForUkraine: 16 users\n",
      "  AskARussian: 1047 users\n",
      "  AskReddit: 200 users\n",
      "  CombatFootage: 2727 users\n",
      "Total nodes in the sampled users: 36805\n"
     ]
    }
   ],
   "source": [
    "def sample_users_by_subreddit(df, sample_rate=0.05, random_seed=42):\n",
    "    \"\"\"\n",
    "    Group users by subreddit and sample a percentage of users from each subreddit.\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): Cleaned dataframe with unique user-subreddit combinations\n",
    "        sample_rate (float): Percentage of users to sample from each subreddit (0.0-1.0)\n",
    "        random_seed (int): Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        pandas.Series: Series of lists containing sampled users for each subreddit\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set random seed for reproducibility\n",
    "        np.random.seed(random_seed)\n",
    "        \n",
    "        # Group users by subreddit\n",
    "        subreddit_users = df.groupby('subreddit')['author_name'].apply(list)\n",
    "        \n",
    "        # Sample users from each subreddit\n",
    "        subreddit_users_sampled = subreddit_users.apply(lambda users: \n",
    "            np.random.choice(users, size=max(1, int(len(users) * sample_rate)), replace=False).tolist()\n",
    "        )\n",
    "        \n",
    "        print(f\"Sampled {sample_rate*100}% of users from each subreddit\")\n",
    "        print(f\"Example of sampled subreddits:\")\n",
    "        for i, (subreddit, users) in enumerate(subreddit_users_sampled.head().items()):\n",
    "            print(f\"  {subreddit}: {len(users)} users\")\n",
    "            if i >= 4:\n",
    "                break\n",
    "                \n",
    "        return subreddit_users_sampled\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in sample_users_by_subreddit: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Sample users by subreddit\n",
    "sampled_users = sample_users_by_subreddit(df_clean, sample_rate=0.05)\n",
    "# Total nodes in the sample\n",
    "print(\"Total nodes in the sampled users:\", sum(len(users) for users in sampled_users))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Network Edge List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 4/48 subreddits processed (8.3%)\n",
      "Progress: 8/48 subreddits processed (16.7%)\n",
      "Progress: 12/48 subreddits processed (25.0%)\n",
      "Progress: 16/48 subreddits processed (33.3%)\n",
      "Progress: 20/48 subreddits processed (41.7%)\n",
      "Progress: 24/48 subreddits processed (50.0%)\n",
      "Progress: 28/48 subreddits processed (58.3%)\n",
      "Progress: 32/48 subreddits processed (66.7%)\n",
      "Progress: 36/48 subreddits processed (75.0%)\n",
      "Progress: 40/48 subreddits processed (83.3%)\n",
      "Progress: 44/48 subreddits processed (91.7%)\n",
      "Progress: 48/48 subreddits processed (100.0%)\n",
      "Network edge list created with 64766831 edges and written to 'network.txt'\n"
     ]
    }
   ],
   "source": [
    "def generate_network(sampled_users, output_file='network.txt'):\n",
    "    \"\"\"\n",
    "    Generate a network edge list where edges connect users who have posted in the same subreddit.\n",
    "    \n",
    "    Args:\n",
    "        sampled_users (pandas.Series): Series of lists containing sampled users for each subreddit\n",
    "        output_file (str): Path to the output edge list file\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the created edge list file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        total_edges = 0\n",
    "        total_subreddits = len(sampled_users)\n",
    "        \n",
    "        with open(output_file, 'w') as f:\n",
    "            for i, (subreddit, users) in enumerate(sampled_users.items()):\n",
    "                # Generate all user-user pairs (edges) for this subreddit\n",
    "                subreddit_edges = list(combinations(users, 2))\n",
    "                \n",
    "                # Write edges to file\n",
    "                for u1, u2 in subreddit_edges:\n",
    "                    f.write(f\"{u1} {u2}\\n\")\n",
    "                \n",
    "                total_edges += len(subreddit_edges)\n",
    "                \n",
    "                # Print progress every 10% of subreddits\n",
    "                if (i+1) % max(1, total_subreddits // 10) == 0 or i+1 == total_subreddits:\n",
    "                    print(f\"Progress: {i+1}/{total_subreddits} subreddits processed ({(i+1)/total_subreddits*100:.1f}%)\")\n",
    "        \n",
    "        print(f\"Network edge list created with {total_edges} edges and written to '{output_file}'\")\n",
    "        return output_file\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in generate_network: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Generate the network edge list\n",
    "edge_list_file = generate_network(sampled_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network loaded from network.txt\n",
      "  Number of nodes: 35712\n",
      "  Number of edges: 64757354\n"
     ]
    }
   ],
   "source": [
    "def load_network(edge_list_file):\n",
    "    \"\"\"\n",
    "    Load a network from an edge list file.\n",
    "    \n",
    "    Args:\n",
    "        edge_list_file (str): Path to the edge list file\n",
    "        \n",
    "    Returns:\n",
    "        networkx.Graph: The loaded network graph\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if file exists and has content\n",
    "        if not os.path.exists(edge_list_file) or os.path.getsize(edge_list_file) == 0:\n",
    "            raise FileNotFoundError(f\"Edge list file not found or empty: {edge_list_file}\")\n",
    "        \n",
    "        # Load the network\n",
    "        G = nx.read_edgelist(edge_list_file)\n",
    "        print(f\"Network loaded from {edge_list_file}\")\n",
    "        print(f\"  Number of nodes: {G.number_of_nodes()}\")\n",
    "        print(f\"  Number of edges: {G.number_of_edges()}\")\n",
    "        \n",
    "        return G\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in load_network: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Load the network\n",
    "G = load_network(edge_list_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Network Analysis:\n",
      "  Number of connected components: 6\n",
      "  Size of largest connected component: 35609 nodes\n",
      "  Percentage of nodes in largest component: 99.71%\n"
     ]
    }
   ],
   "source": [
    "def analyze_network(G):\n",
    "    \"\"\"\n",
    "    Perform basic analysis on a network graph.\n",
    "    \n",
    "    Args:\n",
    "        G (networkx.Graph): The network graph to analyze\n",
    "        \n",
    "    Returns:\n",
    "        networkx.Graph: The analyzed network graph\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"\\nNetwork Analysis:\")\n",
    "        \n",
    "        # Connected components analysis\n",
    "        num_components = nx.number_connected_components(G)\n",
    "        print(f\"  Number of connected components: {num_components}\")\n",
    "        \n",
    "        if num_components > 0:\n",
    "            # Get largest connected component\n",
    "            largest_cc = max(nx.connected_components(G), key=len)\n",
    "            largest_cc_size = len(largest_cc)\n",
    "            print(f\"  Size of largest connected component: {largest_cc_size} nodes\")\n",
    "            print(f\"  Percentage of nodes in largest component: {largest_cc_size/G.number_of_nodes()*100:.2f}%\")\n",
    "        \n",
    "        return G\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in analyze_network: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Analyze the network\n",
    "G = analyze_network(G)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
